<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FIReStereo provides synchronized stereo thermal images, ground truth depth maps, LiDAR scans, and odometry data in challenging outdoor environments.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="icon" href="./static/favicon/favicon.ico" type="image/x-icon">
  <link rel="icon" href="./static/favicon/favicon-16x16.png" sizes="16x16" type="image/png">
  <link rel="icon" href="./static/favicon/favicon-32x32.png" sizes="32x32" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FIReStereo: Forest InfraRed Stereo Dataset for UAS
            Depth Perception in Visually Degraded Environments</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/devanshdhrafani/">Devansh Dhrafani</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/yifei-migo-liu/">Yifei Liu</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://andrewjong.github.io/">Andrew Jong</a>,
            </span>
            <span class="author-block">
              <a href="https://ukcheolshin.github.io/">Ukcheol Shin</a>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Yao He</a>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/tylerkharp/">Tyler Harp</a>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=qIs5g7MAAAAJ&hl=en">Yaoyu Hu</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~./jeanoh/">Jean Oh</a>,
            </span>
            <span class="author-block">
              <a href="https://www.ri.cmu.edu/ri-faculty/sebastian-scherer/">Sebastian Scherer</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Carnegie Mellon University</span>
          </div>
          <div class="is-size-8 publication-authors">
            <span class="author-block"><sup>*</sup>Denotes equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/firestereo/firestereo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
              <!-- Sample Dataset Link. -->
              <span class="link-block">
                <a href="static/FIReStereo_sample.zip"
                   class="external-link button is-normal is-rounded is-dark" download>
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Sample Data</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/firestereo/firestereo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline controls height="100%">
        <source src="./static/videos/teaser_final.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <!-- <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits. -->
        FIReStereo provides synchronized stereo thermal images, ground truth depth maps, LiDAR scans, and odometry data
        in challenging outdoor environments.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Robust depth perception in degraded visual environments is crucial for autonomous aerial 
            systems. Thermal imaging cameras, which capture infrared radiation, are robust to visual 
            degradation. However, due to lack of a large-scale dataset, the use of thermal cameras for 
            unmanned aerial system (UAS) depth perception has remained largely unexplored. This paper 
            presents a stereo thermal depth perception dataset for autonomous aerial perception 
            applications. The dataset consists of stereo thermal images, LiDAR, IMU and ground truth 
            depth maps captured in urban and forest settings under diverse conditions like day, night, 
            rain, and smoke. We benchmark representative stereo depth estimation algorithms, offering 
            insights into their performance in degraded conditions. Models trained on our dataset 
            generalize well to unseen smoky conditions, highlighting the robustness of stereo thermal 
            imaging for depth perception. We aim for this work to enhance robotic perception in 
            disaster scenarios, allowing for exploration and operations in previously unreachable areas.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Hardware. -->
        <h2 class="title is-4">Hardware setup</h2>
        <div class="content has-text-justified">
          <p>
            A pair of stereo thermal cameras, a LiDAR, and an inertial measurement unit (IMU) are 
            mounted on an unmanned aerial vehicle (UAV) platform, which supports 
            data collection during handheld experiments and UAV flights. The stereo thermal pair is 
            facing forward with a 24.6 cm baseline, and the LiDAR is positioned 
            on top of the UAV. An onboard NVIDIA® Jetson AGX Orin™ computer is
            connected to the sensors. Setup and coordinate system for each sensor:
          </p>
          <img src="./static/images/hardware_setup_coord.png"
              class="hardware_system"
              alt="Hardware setup and coordinate system for data collection"/>
          <p>Sensor specifications: </p>
          <img src="./static/images/sensors_spec.png"
                 class="sensor_specs"
                 alt="Hardware setup for data collection"
                 style="width: 85%"/>
        </div>
        <!--/ Hardware. -->

        <!--/ Collection. -->
        <h2 class="title is-4">Data Collection</h2>
        <div class="content has-text-justified">
          <p>Depth Map Generation:</p>
          <ul>
            <li>Obtain calibration for Stereo thermal - IMU and LiDAR - IMU.</li>
            <li>Use LiDAR and IMU data for SLAM to estimate UAV trajectory and generate 3D scene reconstruction.</li>
            <li>Interpolate odometry pose to thermal timestamps and obtain camera pose.</li>
            <li>Project point cloud to camera frame using calibration extrinsics.</li>
            <li>Handle occlusion with 2D grid blocking and filter points with left-right consistency.</li>
          </ul>
          <p>The resulting depth map is in the <strong>left</strong> camera frame, and stereo disparity
            can be obtained with provided calibration, supporting both monocular and stereo depth estimation.
            A closed-loop trajectory was followed with the same initial and final position, making the dataset
            suitable for testing loop closure and accumulated drift for mapping and localization.</p>
          <img src="./static/images/slam_maps.png"
            class="slam"
            alt="Maps generated by Faster-LIO"/>
            The data includes recordings from 4 distinct locations and 16 unique trajectories under various
            environmental conditions, including day, night, rain, cloud cover, smoke. Smoke was emitted from training-grade smoke pots,
            with one location featuring smoke from an actual prescribed fire.
          </p>
          <img src="./static/images/env_photos.png"
            class="data_env"
            alt="Photos of various environments."/>

        </div>
        <!--/ Collection. -->

        <h2 class="title is-4">Data Description</h2>
        <div class="content has-text-justified">
          <p>
            <img src="./static/images/depth_histogram.png" alt="Sample Image" style="float: right; margin-left: 10px; width: 250px;">
            The processed FIReStereo dataset contains 204,594 stereo thermal
            images total across all environments. 29% are in urban
            environment, 15% are in mixed environment, 56% are
            in wilderness environment with dense trees. 84% of the
            images were collected in day-time and the rest were during
            night-time. Obstacles were measured at a median depth of 7.40 m, with quartiles q1 =
            5.17 m and q3 = 10.52 m, which falls within the typical
            range for UAS obstacle avoidance. Histogram on the right shows the distribution of distances to objects.
            42% stereo thermal pairs are smokeless, while 58% contain smoke. Of the smokeless
            images, 35,706 have corresponding depth-map pairs.
          </p>
          <p>Our data various in environment conditions and varying amounts of clutter spanning sub-urban settings, sparse trees, and dense trees. 
            Depth data is available for the first two locations, while the latter two are intended for testing purposes. A detailed description of each sequence in the 4 locations can be found in the dataset text file.</p>
          <ul>
            <li>Hawkins: Collected on a cloudy and windy day, featuring scenes of dense forest and urban structures.</li>
            <li>Frick: Collected on a rainy night, featuring bare trees in varying sparity and urban objects.</li>
            <li>Gascola: Collected in heavily degraded wilderness during day and night, featuring dense smoke and vegetation to simulate wildfire disaster response scenario.</li>
            <li>Firesgl: Collected during an actual prescribed fire, featuring dense forest, smoke, and fire.</li>
          </ul>
          <img src="./static/images/data_variety.png"
              class="data"
              alt="Data variety spanning urban settings, sparse trees, dense trees"/>
          <p>Noticebly, flames, fire embers, and objects relevant to diaster response are visible in the Firesgl collection, making it useful for developing algorithms like ember detection for wildfire monitoring.</p>
          <img src="./static/images/prescribed_data.png"
          class="data"
          alt="Data collected in UAV flight in prescribed fire"/>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column has-text-centered">
        <div class="content">
          <video id="ember" autoplay muted loop playsinline controls height="100%">
            <source src="./static/videos/ember.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column has-text-centered">
        <div class="content">
          <video id="ember" autoplay muted loop playsinline controls height="100%">
            <source src="./static/videos/ember2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <div class="column has-text-centered">
        <div class="content">
          <video id="ember" autoplay muted loop playsinline controls height="100%">
            <source src="./static/videos/ember3.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Evaluation</h2>
        <div class="content has-text-justified">
          <p>We implemented 5 representative stereo depth estimation
            models to evaluate the capabilities of our new dataset in
            facilitating robust depth estimation for UAS navigation in
            cluttered environment. More details and quantitative results 
            can be found in the paper.
          </p>
          <ul>
            <li>Lightweight networks: Fast-ACVNet, MobileStereoNet</li>
            <li>3D networks: AANet, GWCNet, PSMNet</li>
          </ul>
          <p>Fast-ACVNet is used to generate qualitative results, as it is best 
            suited for running on a low Size, Weight, Power, and Cost (SWaP-C)
            system while maintaining similar performance to the more
            resource-intensive models. We observe the model trained on our dataset
            is now able to estimate depth for outdoor thermal images with 
            challenging objects, such as thin tree branches and poles, where were
            previously difficult to capture.
          </p>
          <img src="./static/images/inference.png"
              class="depth_result"
              alt="Depth estimation results"/>
          <p>We further evaluate the trained model on unseen environment with highly dense smoke conditions. 
            Results show than the model trained on smokeless data is able to generalize to these smoke-filled environments.</p>
          <img src="./static/images/smoke_infer.png"
              class="depth_result"
              alt="Depth estimation results in smoke-filled environment"/>
        </div>
      </div>
    </div>

    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{firestereo,
  author    = {Dhrafani, Devansh and Liu, Yifei and Jong, Andrew and Shin, Ukcheol and He, Yao and Harp, Tyler and Hu, Yaoyu and Oh, Jean and Scherer, Sebastian},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  eprint    = {x},
  archivePrefix={arXiv},
  year      = {2024},
  primaryClass = {cs.CV}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from the <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> template,
            which is licensed under a<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
            Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
