<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FIReStereo-sUAS: Forest InfraRed Stereo Dataset for small UAS Depth Perception in Degraded Visual Environments</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->
  <link rel="icon" href="./static/favicon/favicon.ico" type="image/x-icon">
  <link rel="icon" href="./static/favicon/favicon-16x16.png" sizes="16x16" type="image/png">
  <link rel="icon" href="./static/favicon/favicon-32x32.png" sizes="32x32" type="image/png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FIReStereo-sUAS: Forest InfraRed Stereo Dataset for small UAS
            Depth Perception in Degraded Visual Environments</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/devanshdhrafani/">Devansh Dhrafani</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/yifei-migo-liu/">Yifei Liu</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://andrewjong.github.io/">Andrew Jong</a>,
            </span>
            <span class="author-block">
              <a href="https://ukcheolshin.github.io/">Ukcheol Shin</a>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Yao He</a>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/tylerkharp/">Tyler Harp</a>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/yaoyu-hu-68946985/">Yaoyu Hu</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~./jeanoh/">Jean Oh</a>,
            </span>
            <span class="author-block">
              <a href="https://www.ri.cmu.edu/ri-faculty/sebastian-scherer/">Sebastian Scherer</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Carnegie Mellon University</span>
          </div>
          <div class="is-size-8 publication-authors">
            <span class="author-block"><sup>*</sup>Denotes equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline controls height="100%">
        <source src="./static/videos/teaser_final.mp4"
                type="video/mp4">
      </video>
      <!-- <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2> -->
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Robust depth perception in degraded visual environments is crucial for autonomous aerial 
            systems. Thermal imaging cameras, which capture infrared radiation, are robust to visual 
            degradation. However, due to lack of a large-scale dataset, the use of thermal cameras for 
            unmanned aerial system (UAS) depth perception has remained largely unexplored. This paper 
            presents a stereo thermal depth perception dataset for autonomous aerial perception 
            applications. The dataset consists of stereo thermal images, LiDAR, IMU and ground truth 
            depth maps captured in urban and forest settings under diverse conditions like day, night, 
            rain, and smoke. We benchmark representative stereo depth estimation algorithms, offering 
            insights into their performance in degraded conditions. Models trained on our dataset 
            generalize well to unseen smoky conditions, highlighting the robustness of stereo thermal 
            imaging for depth perception. We aim for this work to enhance robotic perception in 
            disaster scenarios, allowing for exploration and operations in previously unreachable areas.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Hardware setup</h3>
        <div class="content has-text-justified">
          <p>
            We designed a data collection platform comprising a pair of stereo thermal cameras, 
            a LiDAR, and an inertial measurement unit (IMU), as illustrated in the diagram below. 
            These sensors are mounted on an unmanned aerial vehicle (UAV) platform, which supports 
            data collection during handheld experiments and UAV flights. The stereo thermal pair is 
            oriented in a forward-facing direction with a 24.6 cm baseline, and the LiDAR is positioned 
            on top of the UAV. An onboard NVIDIA® Jetson AGX Orin™ computer is
            connected to the sensors. To ensure clarity in interpreting the orientation and relative 
            positioning of the sensors, we provide a diagram detailing the coordinate systems for each 
            sensor, which is crucial for subsequent sensor fusion and data processing.
        </div>
        <div class="content has-text-justified">
          <!-- <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video> -->
          <img src="./static/images/hardware_setup_coord.png"
              class="hardware_system"
              alt="Hardware setup and coordinate system for data collection"/>
          <p>Sensor specifications are presented below. For the FLIR Boson thermal cameras, 
            we capture the raw-16-bit data, as it retains detailed temperature information 
            that is lost in 8-bit RGB formats.</p>
          <img src="./static/images/sensors_spec.png"
                 class="sensor_specs"
                 alt="Hardware setup for data collection"
                 style="width: 80%"/>
        </div>
        <!--/ Collection. -->
        <h3 class="title is-4">Data Collection</h3>
        <div class="content has-text-justified">
          <ul>
            <li>Camera-IMU calibration: Kalibr with heated checkerboard target.</li>
            <li>LiDAR SLAM: Faster-LIO.</li>
            <li>Pipeline: LiDAR scans -> Scene reconstruction -> Projection to image frame 
              -> Occlusion handling & Filtering.</li>
          </ul>
          <p></p>
            The data includes recordings from 3
            distinct locations and 13 unique trajectories under various
            environmental conditions, including day, night, rain, cloud
            cover, and smoke. Smoke was emitted from training-grade
            smoke pots. All collected sensor data is time-synchronized
            with the CPU clock, making it also suitable to use for mapping 
            and localization. A closed-loop trajectory was followed
            with the same initial and final position, making the dataset
            suitable for testing loop closure and accumulated drift for
            mapping and localization. Example of such trajectories with reconstructed
            scenes are shown below.
          </p>
          <img src="./static/images/slam_maps.png"
                 class="slam"
                 alt="Maps generated by Faster-LIO"/>
        </div>
        <!--/ Collection. -->

        <h3 class="title is-4">Data Description</h3>
        <div class="content has-text-justified">
          <p>
            <strong>Quantative:</strong> The processed
            FIReStereo-sUAS dataset contains 150,579 stereo thermal
            images total across all environments. 39% are in urban
            environment, 21% are in mixed environment, 40% are
            in wilderness environment with dense trees. 84% of the
            images were collected in day-time and the rest were during
            night-time. Obstacles were measured at a median depth of
            7.40 m which is a typical range for UAS obstacle avoidance. 
            48% stereo thermal pairs are smokeless, while 52% contain smoke. Of the smokeless
            images, we label 35,706 with depth-map pairs annotated
            from LiDAR and Faster-LIO.
          </p>
          <p>
            <strong>Qualitative:</strong> The Hawkins
            experimental sequences feature scenes of dense forests and
            urban structures. The data were collected during the day with
            cloudy and windy conditions. (YL: should we include below details?)
            <ul>
              <li>Sequence 1-2: Features dense trees and branches with varying thickness.</li>
              <li>Sequence 3: Captures views of a thin pole, trees, and buildings representing
                typical urban obstacles that a UAS might face in response to a wildfire disaster.</li>
              <li>Sequence 4: Contains views of a car, pole, and distant trees.</li>
              <li>Sequence 5: replicated a disaster scenario of an upside-down car engulfed in dense smoke.</li>
              </ul> 
          </p>
          <p>
            The Frick experimental sequences were recorded during
            the night and under rainy conditions. The captured temperature range for these sequences is much lower and is
            evident from the darker thermal images. These sequences
            feature bare trees in varying sparsity, vehicles, poles, roads
            and buildings.
          </p>
          <p>
            Gascola sequences were recorded in heavily degraded wilderness, featuring dense smoke, night-time,
            dense trees and bushes. These conditions were chosen to
            simulate the wildfire disaster response scenario in which the
            UAS must navigate through a cluttered forest environment
            with extreme visual degradation. We show that depth estimation models trained on smokeless data is able to generalize
            to these smoke-filled data.
          </p>
          <p>The figure below shows the variety of environments spanning urban settings, sparse trees, and dense trees.</p>
          <img src="./static/images/data_variety.png"
              class="data"
              alt="Data variety spanning urban settings, sparse trees, dense trees"/>
          <p>(YL: todo prescribed fire thermal)</p>
        </div>

        <h3 class="title is-4">Evaluation</h3>
        <div class="content has-text-justified">
          <p>We implemented 5 representative stereo depth estimation
            models to evaluate the capabilities of our new dataset in
            facilitating robust depth estimation for UAS navigation in
            cluttered environment. More details and quantative results 
            can be found in the paper.
          </p>
          <ul>
            <li>Lightweight networks: Fast-ACVNet, MobileStereoNet</li>
            <li>3D networks: AANet, GWCNet, PSMNet</li>
          </ul>
          <p>Fast-ACVNet is used to generate qualitative results, as it is best 
            suited for running on a low Size, Weight, Power, and Cost (SWaP-C)
            system while maintaining similar performance to the more
            resource-intensive models. We observe the model trained on our dataset
            is now able to estimate depth for outdoor thermal images with 
            challenging objects such as thin tree branches and poles. (YL: wording?)
          </p>
          <img src="./static/images/inference.png"
              class="depth_result"
              alt="Depth estimation results"/>
          <p>We further evaluate the trained model on unseen environment with highly dense smoke conditions. 
            Results show than the model trained on smokeless data is able to generalize to these smoke-filled environments.</p>
          <img src="./static/images/smoke_infer.png"
              class="depth_result"
              alt="Depth estimation results in smoke-filled environment"/>
          </div>

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{fisData,
  author    = {Dhrafani, Devansh and Liu, Yifei and Jong, Andrew and Shin, Ukcheol and He, Yao and Harp, Tyler and Hu, Yaoyu and Oh, Jean and Scherer, Sebastian},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  eprint    = {x},
  archivePrefix={arXiv},
  year      = {2024},
  primaryClass = {cs.CV}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from the <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> template,
            which is licensed under a<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
            Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
